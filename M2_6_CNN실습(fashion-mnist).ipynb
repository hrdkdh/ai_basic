{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 모듈 불러오기\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-05T09:55:05.183758Z","iopub.execute_input":"2022-04-05T09:55:05.184129Z","iopub.status.idle":"2022-04-05T09:55:09.223769Z","shell.execute_reply.started":"2022-04-05T09:55:05.184046Z","shell.execute_reply":"2022-04-05T09:55:09.222987Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# FASHION MNIST 데이터셋 불러오기\nfashion_mnist = tf.keras.datasets.fashion_mnist\n(train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()\n\n# 데이터셋 정규화\ntrain_X = train_X / 255.0\ntest_X = test_X / 255.0\n\n# 데이터셋 shape 확인\nprint(train_X.shape, test_X.shape)\n\n# 데이터셋에 차원(채널) 추가\ntrain_X = train_X.reshape(60000, 28, 28, 1) # 2차원(28*28)에서 3차원(28*28*1)으로 변환\ntest_X = test_X.reshape(10000, 28, 28, 1) # 2차원(28*28)에서 3차원(28*28*1)으로 변환\n\n# reshape 이후 shape 확인\nprint(train_X.shape, test_X.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-05T09:55:09.225267Z","iopub.execute_input":"2022-04-05T09:55:09.225522Z","iopub.status.idle":"2022-04-05T09:55:11.460544Z","shell.execute_reply.started":"2022-04-05T09:55:09.225487Z","shell.execute_reply":"2022-04-05T09:55:11.459799Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 데이터 확인\nplt.figure(figsize=(10, 10))  #전체 그래프 사이즈를 width=10, height=10 으로 지정\nfor c in range(16):\n    plt.subplot(4, 4, c+1)  #4행 4열로 지정한 grid 에서 c+1 번째의 칸에 그래프 그림. \n    plt.imshow(train_X[c].reshape(28,28), cmap=\"gray\")    \nplt.show()\n\n#train 데이터의 첫번째 ~ 16번째 까지의 라벨을 프린트\nprint(train_Y[:16])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T09:55:11.462070Z","iopub.execute_input":"2022-04-05T09:55:11.462330Z","iopub.status.idle":"2022-04-05T09:55:12.637741Z","shell.execute_reply.started":"2022-04-05T09:55:11.462295Z","shell.execute_reply":"2022-04-05T09:55:12.637071Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 컨볼루션 신경망 모델 정의\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(input_shape=(28, 28, 1), kernel_size=(3, 3), filters=16),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=32),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=64),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# 계층, 차원, 파라미터 수 요약 확인\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T09:55:12.639729Z","iopub.execute_input":"2022-04-05T09:55:12.640426Z","iopub.status.idle":"2022-04-05T09:55:15.073570Z","shell.execute_reply.started":"2022-04-05T09:55:12.640387Z","shell.execute_reply":"2022-04-05T09:55:15.072885Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 학습실행 : 학습데이터에서 검증 데이터 25% 분리, epochs 25회 후 손실 및 정확도 출력\nhistory = model.fit(train_X, train_Y, epochs=25, validation_split=0.25)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T09:55:15.074797Z","iopub.execute_input":"2022-04-05T09:55:15.075043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습결과 확인 : train loss, validation loss 그래프 출력\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"], \"b-\", label=\"loss\")\nplt.plot(history.history[\"val_loss\"], \"r--\", label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.legend()\n\n# 학습결과 확인 : train accuracy, validation accuracy 그래프 출력\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"], \"g-\", label=\"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], \"k--\", label=\"val_accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylim(0.7, 1)\nplt.legend()\nplt.show()\n\n# 테스트 데이터에 대한 손실 및 정확도 출력\nmodel.evaluate(test_X, test_Y, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 에포크가 증가할 수록 validation loss가 증가하고, validation accuracy는 감소하고 있어 과적합 상태임\n# test data에 대한 loss와 accuracy가 train data에 비해 낮음\n# 성능 향상을 위해 모델 재설계 필요\n\n# 과적합 해소를 위해 pooling layer, drop out 추가\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(input_shape=(28, 28, 1), kernel_size=(3, 3), filters=32),\n    tf.keras.layers.MaxPool2D(strides=(2, 2)),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=64),\n    tf.keras.layers.MaxPool2D(strides=(2, 2)),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=128),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n    tf.keras.layers.Dropout(rate=0.3),\n    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습실행 : 학습데이터에서 검증 데이터 25% 분리, epochs 25회 후 손실 및 정확도 출력\nhistory = model.fit(train_X, train_Y, epochs=25, validation_split=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습결과 확인 : train loss, validation loss 그래프 출력\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"], \"b-\", label=\"loss\")\nplt.plot(history.history[\"val_loss\"], \"r--\", label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.legend()\n\n# 학습결과 확인 : train accuracy, validation accuracy 그래프 출력\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"], \"g-\", label=\"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], \"k--\", label=\"val_accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylim(0.7, 1)\nplt.legend()\nplt.show()\n\n# 테스트 데이터에 대한 손실 및 정확도 출력\nmodel.evaluate(test_X, test_Y, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VGGnet 유형으로 모델 정의  제로패딩, activation relu 사용\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(input_shape=(28, 28,1), kernel_size=(3, 3), filters=32, padding=\"same\", activation=\"relu\"),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=64, padding=\"same\", activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=128, padding=\"same\", activation=\"relu\"),\n    tf.keras.layers.Conv2D(kernel_size=(3, 3), filters=256, padding= \"same\", activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.Dense(units=256, activation=\"relu\"),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=\"sparse_categorical_crossentropy\", \n              metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습실행 : 학습데이터에서 검증 데이터 25% 분리, epochs 25회 후 손실 및 정확도 출력\nhistory = model.fit(train_X, train_Y, epochs=25, validation_split=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습결과 확인 : train loss, validation loss 그래프 출력\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"], \"b-\", label=\"loss\")\nplt.plot(history.history[\"val_loss\"], \"r--\", label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.legend()\n\n# 학습결과 확인 : train accuracy, validation accuracy 그래프 출력\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"], \"g-\", label=\"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], \"k--\", label=\"val_accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylim(0.7, 1)\nplt.legend()\nplt.show()\n\n# 테스트 데이터에 대한 손실 및 정확도 출력\nmodel.evaluate(test_X, test_Y, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}